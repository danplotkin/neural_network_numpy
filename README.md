# Building and Training Neural Networks with NumPy from Scratch

# About

In this project, I will dive into the core principles of neural networks, starting with basic building blocks and progressively constructing more complex networks. I will be using just NumPy to implement a feed-forward nerual network class that can handle a range of inputs, which is useful for devolping deep learning libraries like Keras and PyTorch, as well as designing the architecture for the layers and training. By leveraging NumPy, a fundamental library for numerical computing in Python, I guide readers through the process of creating custom neural networks, layer by layer, and training them for various tasks.

From data preprocessing to forward and backward propagation, I demystify the key components of neural networks, shedding light on activation functions, weight initialization, and gradient descent optimization. Through practical exercises and code examples, I aim to make deep learning accessible and comprehensible. I will be using the MNIST dataset that comes with Google Colab.

# Results

The results for the final network created in NumPy had 92.55% accuracy to validation data when trained for 50 epochs.

# Project Notebook

[View in GitHub](https://github.com/danplotkin/neural_network_numpy/blob/main/NeuralNetworkNumPy.ipynb)

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/danplotkin/neural_network_numpy/blob/main/NeuralNetworkNumPy.ipynb)
